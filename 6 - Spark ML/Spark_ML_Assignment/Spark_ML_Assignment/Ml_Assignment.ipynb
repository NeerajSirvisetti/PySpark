{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Average Purchase amount?\n",
    "2. Counting and Removing null values\n",
    "3. How many distinct values per column?\n",
    "4. Count category values within each of the following column:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status\n",
    "5. Calculate average Purchase for each of the following columns:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status\n",
    "6. Label encode the following columns:\n",
    "     - Age\n",
    "     - Gender\n",
    "     - Stay_In_Current_City_Years\n",
    "     - City_Category\n",
    "7. One-Hot encode following columns:\n",
    "     - Gender\n",
    "     - City_Category\n",
    "     - Occupation\n",
    "8. Build a baseline model using any of the ML algorithms.\n",
    "9. Model improvement with Grid-Search CV\n",
    "10. Create a Spark ML Pipeline for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary libraries\n",
    "# importing the required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 08:53:16 WARN Utils: Your hostname, codespaces-ae9593 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "22/07/04 08:53:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 08:53:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reading the train data  \n",
    "train_data = spark.read.csv(\"train.csv\",inferSchema=True, header=True)\n",
    "\n",
    "# reading the test data\n",
    "test_data  = spark.read.csv(\"test.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumn(\"Product_ID\", F.col(\"Product_ID\").cast(tp.IntegerType()))\n",
    "test_data = test_data.withColumn(\"Product_ID\", F.col(\"Product_ID\").cast(tp.IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Average Purchase amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Purchasing Amount is 9263.9687.\n",
      "+----------------+\n",
      "|Average_Purchase|\n",
      "+----------------+\n",
      "|       9263.9687|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_variable = train_data.agg(F.round(F.avg(\"Purchase\"),4).alias(\"Average_Purchase\"))\n",
    "j = ((target_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Purchasing Amount is {j['Average_Purchase']}.\")\n",
    "target_variable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Counting and Removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product_ID 550068\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 173638\n",
      "Product_Category_3 383247\n",
      "Purchase 0\n"
     ]
    }
   ],
   "source": [
    "# null values in each column\n",
    "for c in train_data.columns:\n",
    "    missing_values = F.isnull(c)\n",
    "    missing_values = train_data.filter(missing_values).count()\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Product_ID Amount is None.\n",
      "The Average Product_Category_2 Amount is 9.8423.\n",
      "The Average Product_Category_3 Amount is 12.6682.\n"
     ]
    }
   ],
   "source": [
    "# Average Product_category_2 and Product_category_3 amount\n",
    "Product_ID_variable = train_data.agg(F.round(F.avg(\"Product_ID\"),4).alias(\"Average_Product_ID\"))\n",
    "j = ((Product_ID_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Product_ID Amount is {j['Average_Product_ID']}.\")\n",
    "value_1 = j['Average_Product_ID']\n",
    "Product_Category_2_variable = train_data.agg(F.round(F.avg(\"Product_Category_2\"),4).alias(\"Average_Product_Category_2\"))\n",
    "j = ((Product_Category_2_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Product_Category_2 Amount is {j['Average_Product_Category_2']}.\")\n",
    "value_2 = j['Average_Product_Category_2']\n",
    "Product_Category_3_variable = train_data.agg(F.round(F.avg(\"Product_Category_3\"),4).alias(\"Average_Product_Category_3\"))\n",
    "j = ((Product_Category_3_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Product_Category_3 Amount is {j['Average_Product_Category_3']}.\")\n",
    "value_3 = j['Average_Product_Category_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to Replaces the missing_values with the Averages of their respective column\n",
    "train_data = train_data.fillna({\"Product_ID\":0,\"Product_Category_2\": value_2, \"Product_Category_3\" : value_3})\n",
    "test_data = test_data.fillna({\"Product_ID\":0,\"Product_Category_2\": value_2, \"Product_Category_3\" : value_3})\n",
    "\n",
    "train_data = train_data.withColumn(\"Product_Category_2\", F.col(\"Product_Category_2\").cast(tp.IntegerType()))\n",
    "train_data = train_data.withColumn(\"Product_Category_3\", F.col(\"Product_Category_3\").cast(tp.IntegerType()))\n",
    "\n",
    "\n",
    "test_data = test_data.withColumn(\"Product_Category_2\", F.col(\"Product_Category_2\").cast(tp.IntegerType()))\n",
    "test_data = test_data.withColumn(\"Product_Category_3\", F.col(\"Product_Category_3\").cast(tp.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n",
      "Product_ID 0\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 0\n",
      "Product_Category_3 0\n",
      "Purchase 0\n"
     ]
    }
   ],
   "source": [
    "# null values in each column\n",
    "for c in train_data.columns:\n",
    "    missing_values = F.isnull(c)\n",
    "    missing_values = train_data.filter(missing_values).count()\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How many distinct values per column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|   5891|         1|     2|  7|        21|            3|                         5|             2|                20|                17|                15|   18105|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# distinct values in each column\n",
    "train_data.agg(*(F.countDistinct(F.col(c)).alias(c) for c in train_data.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Count category values within each of the following column:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Gender| count|\n",
      "+------+------+\n",
      "|     F|135809|\n",
      "|     M|414259|\n",
      "+------+------+\n",
      "\n",
      "+-----+------+\n",
      "|  Age| count|\n",
      "+-----+------+\n",
      "|18-25| 99660|\n",
      "|26-35|219587|\n",
      "| 0-17| 15102|\n",
      "|46-50| 45701|\n",
      "|51-55| 38501|\n",
      "|36-45|110013|\n",
      "|  55+| 21504|\n",
      "+-----+------+\n",
      "\n",
      "+-------------+------+\n",
      "|City_Category| count|\n",
      "+-------------+------+\n",
      "|            B|231173|\n",
      "|            C|171175|\n",
      "|            A|147720|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Gender', 'Age','City_Category']:\n",
    "    variable = train_data.groupBy(c).agg(F.count(c).alias(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------+\n",
      "|Stay_In_Current_City_Years| count|\n",
      "+--------------------------+------+\n",
      "|                         3| 95285|\n",
      "|                         0| 74398|\n",
      "|                        4+| 84726|\n",
      "|                         1|193821|\n",
      "|                         2|101838|\n",
      "+--------------------------+------+\n",
      "\n",
      "+--------------+------+\n",
      "|Marital_Status| count|\n",
      "+--------------+------+\n",
      "|             1|225337|\n",
      "|             0|324731|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Stay_In_Current_City_Years','Marital_Status']:\n",
    "    variable = train_data.groupBy(c).agg(F.count(c).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate average Purchase for each of the following columns:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|Gender|Average_Purchase|\n",
      "+------+----------------+\n",
      "|     F|       8734.5658|\n",
      "|     M|        9437.526|\n",
      "+------+----------------+\n",
      "\n",
      "+-----+----------------+\n",
      "|  Age|Average_Purchase|\n",
      "+-----+----------------+\n",
      "|18-25|       9169.6636|\n",
      "|26-35|       9252.6906|\n",
      "| 0-17|       8933.4646|\n",
      "|46-50|       9208.6257|\n",
      "|51-55|        9534.808|\n",
      "|36-45|       9331.3507|\n",
      "|  55+|       9336.2805|\n",
      "+-----+----------------+\n",
      "\n",
      "+-------------+----------------+\n",
      "|City_Category|Average_Purchase|\n",
      "+-------------+----------------+\n",
      "|            B|       9151.3006|\n",
      "|            C|        9719.921|\n",
      "|            A|       8911.9392|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Gender', 'Age','City_Category']:\n",
    "    (train_data.groupBy(c)).agg(F.round(F.avg('Purchase'),4).alias(\"Average_Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------+\n",
      "|Stay_In_Current_City_Years|Average_Purchase|\n",
      "+--------------------------+----------------+\n",
      "|                         3|       9286.9041|\n",
      "|                         0|       9180.0751|\n",
      "|                        4+|       9275.5989|\n",
      "|                         1|       9250.1459|\n",
      "|                         2|       9320.4298|\n",
      "+--------------------------+----------------+\n",
      "\n",
      "+--------------+----------------+\n",
      "|Marital_Status|Average_Purchase|\n",
      "+--------------+----------------+\n",
      "|             1|       9261.1746|\n",
      "|             0|       9265.9076|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Stay_In_Current_City_Years','Marital_Status']:\n",
    "    (train_data.groupBy(c)).agg(F.round(F.avg('Purchase'),4).alias(\"Average_Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Label encode the following columns:\n",
    "     - Age\n",
    "     - Gender\n",
    "     - Stay_In_Current_City_Years\n",
    "     - City_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode \n",
    "SI_AGE = StringIndexer(inputCol= \"Age\", outputCol= \"Age_le\" , handleInvalid=\"skip\")\n",
    "SI_Gender = StringIndexer(inputCol= \"Gender\", outputCol= \"Gender_le\", handleInvalid= \"skip\")\n",
    "SI_Stay_In_Current_City_Years  = StringIndexer(inputCol= \"Stay_In_Current_City_Years\", outputCol= \"Stay_In_Current_City_Years_le\", handleInvalid= \"skip\")\n",
    "SI_City_Category = StringIndexer(inputCol= \"City_Category\", outputCol= \"City_Category_le\", handleInvalid= \"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the objects\n",
    "# label encode objects\n",
    "SI_AGE_Obj = SI_AGE.fit(train_data)\n",
    "SI_Gender_Obj = SI_Gender.fit(train_data)\n",
    "SI_Stay_In_Current_City_Years_Obj = SI_Stay_In_Current_City_Years.fit(train_data)\n",
    "SI_City_Category_Obj = SI_City_Category.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform on training data\n",
    "train_data_encoded = SI_AGE_Obj.transform(train_data)\n",
    "train_data_encoded = SI_Gender_Obj.transform(train_data_encoded)\n",
    "train_data_encoded = SI_Stay_In_Current_City_Years_Obj.transform(train_data_encoded)\n",
    "train_data_encoded = SI_City_Category_Obj.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform on test data\n",
    "test_data_encoded = SI_AGE_Obj.transform(test_data)\n",
    "test_data_encoded = SI_Gender_Obj.transform(test_data_encoded)\n",
    "test_data_encoded = SI_Stay_In_Current_City_Years_Obj.transform(test_data_encoded)\n",
    "test_data_encoded = SI_City_Category_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3',\n",
       " 'Purchase',\n",
       " 'Age_le',\n",
       " 'Gender_le',\n",
       " 'Stay_In_Current_City_Years_le',\n",
       " 'City_Category_le']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_encoded.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. One-Hot encode following columns:\n",
    "     - Gender\n",
    "     - City_Category\n",
    "     - Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding Occupation\n",
    "SI_Occupation = StringIndexer(inputCol= \"Occupation\", outputCol= \"Occupation_le\", handleInvalid= \"skip\")\n",
    "SI_Occupation_Obj = SI_Occupation.fit(train_data)\n",
    "train_data_encoded = SI_Occupation_Obj.transform(train_data_encoded)\n",
    "test_data_encoded = SI_Occupation_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "OHE_train = OneHotEncoder(inputCols=['Gender_le','City_Category_le','Occupation_le'],\n",
    "                                  outputCols=['Gender_le_ohe','City_Category_le_ohe','Occupation_le_ohe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE object\n",
    "OHE_Obj = OHE_train.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform train data\n",
    "train_data_encoded = OHE_Obj.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-------------+--------------------+-----------------+\n",
      "|Marital_Status|Age_le|Gender_le_ohe|City_Category_le_ohe|Occupation_le_ohe|\n",
      "+--------------+------+-------------+--------------------+-----------------+\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   5.0|(1,[0],[1.0])|       (2,[1],[1.0])|   (20,[9],[1.0])|\n",
      "|             0|   0.0|(1,[0],[1.0])|           (2,[],[])|  (20,[14],[1.0])|\n",
      "|             1|   3.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[2],[1.0])|\n",
      "|             1|   3.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[2],[1.0])|\n",
      "|             1|   3.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[2],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             1|   1.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[3],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|       (2,[1],[1.0])|   (20,[6],[1.0])|\n",
      "+--------------+------+-------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the one hot encoded data\n",
    "train_data_encoded.select('Marital_Status','Age_le','Gender_le_ohe','City_Category_le_ohe','Occupation_le_ohe').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = OHE_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Build a baseline model using any of the ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3',\n",
       " 'Purchase',\n",
       " 'Age_le',\n",
       " 'Gender_le',\n",
       " 'Stay_In_Current_City_Years_le',\n",
       " 'City_Category_le',\n",
       " 'Occupation_le',\n",
       " 'Gender_le_ohe',\n",
       " 'City_Category_le_ohe',\n",
       " 'Occupation_le_ohe']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## columns in the dataset\n",
    "train_data_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create feature vector\n",
    "feature_vector = VectorAssembler(inputCols= ['User_ID',\n",
    " 'Product_ID',\n",
    " 'Marital_Status',\n",
    " 'Product_Category_1',\n",
    " 'Product_Category_2',\n",
    " 'Product_Category_3',\n",
    " 'Age_le',\n",
    " 'Stay_In_Current_City_Years_le',\n",
    " 'Gender_le_ohe',\n",
    " 'City_Category_le_ohe',\n",
    " 'Occupation_le_ohe'], outputCol= 'feature_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the feature vector\n",
    "train_data_encoded = feature_vector.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      feature_vector|\n",
      "+--------------------+\n",
      "|(31,[0,3,4,5,6,7,...|\n",
      "|(31,[0,3,4,5,6,7,...|\n",
      "|(31,[0,3,4,5,6,7,...|\n",
      "|(31,[0,3,4,5,6,7,...|\n",
      "|(31,[0,3,4,5,6,7,...|\n",
      "|(31,[0,3,4,5,7,8,...|\n",
      "|(31,[0,2,3,4,5,6,...|\n",
      "|(31,[0,2,3,4,5,6,...|\n",
      "|(31,[0,2,3,4,5,6,...|\n",
      "|(31,[0,2,3,4,5,8,...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the feature vector\n",
    "train_data_encoded.select(\"feature_vector\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: integer (nullable = false)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- Age_le: double (nullable = false)\n",
      " |-- Gender_le: double (nullable = false)\n",
      " |-- Stay_In_Current_City_Years_le: double (nullable = false)\n",
      " |-- City_Category_le: double (nullable = false)\n",
      " |-- Occupation_le: double (nullable = false)\n",
      " |-- Gender_le_ohe: vector (nullable = true)\n",
      " |-- City_Category_le_ohe: vector (nullable = true)\n",
      " |-- Occupation_le_ohe: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = feature_vector.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Building\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DTR = DecisionTreeRegressor(featuresCol='feature_vector', labelCol=\"Purchase\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_DTR = model_DTR.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"Purchase\", metricName=\"rmse\")\n",
    "predictions = model_DTR.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3254.9692382019916"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|Purchase|      feature_vector|\n",
      "+------------------+--------+--------------------+\n",
      "|10635.041568097331|    8370|(31,[0,3,4,5,6,7,...|\n",
      "|13036.571651917404|   15200|(31,[0,3,4,5,6,7,...|\n",
      "| 3644.778734866649|    1422|(31,[0,3,4,5,6,7,...|\n",
      "| 3644.778734866649|    1057|(31,[0,3,4,5,6,7,...|\n",
      "| 8048.940431943747|    7969|(31,[0,3,4,5,6,7,...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"Purchase\", \"feature_vector\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Model improvement with Grid-Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the CrossValidator and ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DTR = DecisionTreeRegressor(featuresCol='feature_vector', labelCol=\"Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(model_DTR.maxDepth, [2, 5, 10])\n",
    "             .addGrid(model_DTR.maxBins, [10, 20, 40])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator( labelCol=\"Purchase\", metricName=\"rmse\")\n",
    "dtcv = CrossValidator(estimator = model_DTR,\n",
    "                      estimatorParamMaps = dtparamGrid,\n",
    "                      evaluator = evaluator,\n",
    "                      numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_11445e1f35e3\n"
     ]
    }
   ],
   "source": [
    "dtcvModel = dtcv.fit(train_data_encoded)\n",
    "print(dtcvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1675:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2925.009142496947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dtpredictions = dtcvModel.transform(train_data_encoded)\n",
    "print('RMSE:', evaluator.evaluate(dtpredictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|Purchase|      feature_vector|\n",
      "+------------------+--------+--------------------+\n",
      "|12317.442622950819|    8370|(31,[0,3,4,5,6,7,...|\n",
      "| 12964.52896921877|   15200|(31,[0,3,4,5,6,7,...|\n",
      "|1577.6288659793815|    1422|(31,[0,3,4,5,6,7,...|\n",
      "|1577.6288659793815|    1057|(31,[0,3,4,5,6,7,...|\n",
      "| 7782.683359877308|    7969|(31,[0,3,4,5,6,7,...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtpredictions.select(\"prediction\", \"Purchase\", \"feature_vector\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the best model parameters dictionary\n",
    "param_dict = dtcvModel.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a filtered dictionary\n",
    "final_dict = {}\n",
    "for k, v in param_dict.items():\n",
    "    final_dict[k.name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cacheNodeIds': False, 'checkpointInterval': 10, 'featuresCol': 'feature_vector', 'impurity': 'variance', 'labelCol': 'Purchase', 'leafCol': '', 'maxBins': 20, 'maxDepth': 10, 'maxMemoryInMB': 256, 'minInfoGain': 0.0, 'minInstancesPerNode': 1, 'minWeightFractionPerNode': 0.0, 'predictionCol': 'prediction', 'seed': -8027478954484760012}\n"
     ]
    }
   ],
   "source": [
    "print(final_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Create a Spark ML Pipeline for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Required Libraries \n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nullValuesTransformer(Transformer):\n",
    "    \n",
    "    def __init__(self, dataframe = None):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def _transform(self, dataframe):\n",
    "        dataframe = dataframe.fillna({\n",
    "            \"Fraud\" : 0,\n",
    "            \"Country\": \"IN\",\n",
    "            \"TrafficType\" : \"U\",\n",
    "            \"Device\": \"Generic\",\n",
    "            \"Browser\": \"chrome\",\n",
    "            \"OS\": \"Android\",\n",
    "        })\n",
    "        \n",
    "        return dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
