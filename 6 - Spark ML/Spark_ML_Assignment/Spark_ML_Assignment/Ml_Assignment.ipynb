{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Average Purchase amount?\n",
    "2. Counting and Removing null values\n",
    "3. How many distinct values per column?\n",
    "4. Count category values within each of the following column:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status\n",
    "5. Calculate average Purchase for each of the following columns:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status\n",
    "6. Label encode the following columns:\n",
    "     - Age\n",
    "     - Gender\n",
    "     - Stay_In_Current_City_Years\n",
    "     - City_Category\n",
    "7. One-Hot encode following columns:\n",
    "     - Gender\n",
    "     - City_Category\n",
    "     - Occupation\n",
    "8. Build a baseline model using any of the ML algorithms.\n",
    "9. Model improvement with Grid-Search CV\n",
    "10. Create a Spark ML Pipeline for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary libraries\n",
    "# importing the required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reading the train data  \n",
    "train_data = spark.read.csv(\"train.csv\",inferSchema=True, header=True)\n",
    "\n",
    "# reading the test data\n",
    "test_data  = spark.read.csv(\"test.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumn(\"Product_ID\", F.col(\"Product_ID\").cast(tp.IntegerType()))\n",
    "test_data = test_data.withColumn(\"Product_ID\", F.col(\"Product_ID\").cast(tp.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Product_ID|\n",
      "+----------+\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "|      null|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.select('Product_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Average Purchase amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Purchasing Amount is 9263.9687.\n",
      "+----------------+\n",
      "|Average_Purchase|\n",
      "+----------------+\n",
      "|       9263.9687|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_variable = train_data.agg(F.round(F.avg(\"Purchase\"),4).alias(\"Average_Purchase\"))\n",
    "j = ((target_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Purchasing Amount is {j['Average_Purchase']}.\")\n",
    "target_variable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Counting and Removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n",
      "Product_ID 550068\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 173638\n",
      "Product_Category_3 383247\n",
      "Purchase 0\n"
     ]
    }
   ],
   "source": [
    "# null values in each column\n",
    "for c in train_data.columns:\n",
    "    missing_values = F.isnull(c)\n",
    "    missing_values = train_data.filter(missing_values).count()\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Product_ID Amount is None.\n",
      "The Average Product_Category_2 Amount is 9.8423.\n",
      "The Average Product_Category_3 Amount is 12.6682.\n"
     ]
    }
   ],
   "source": [
    "# Average Product_category_2 and Product_category_3 amount\n",
    "Product_ID_variable = train_data.agg(F.round(F.avg(\"Product_ID\"),4).alias(\"Average_Product_ID\"))\n",
    "j = ((Product_ID_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Product_ID Amount is {j['Average_Product_ID']}.\")\n",
    "value_1 = j['Average_Product_ID']\n",
    "Product_Category_2_variable = train_data.agg(F.round(F.avg(\"Product_Category_2\"),4).alias(\"Average_Product_Category_2\"))\n",
    "j = ((Product_Category_2_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Product_Category_2 Amount is {j['Average_Product_Category_2']}.\")\n",
    "value_2 = j['Average_Product_Category_2']\n",
    "Product_Category_3_variable = train_data.agg(F.round(F.avg(\"Product_Category_3\"),4).alias(\"Average_Product_Category_3\"))\n",
    "j = ((Product_Category_3_variable.collect())[0]).asDict()\n",
    "print(f\"The Average Product_Category_3 Amount is {j['Average_Product_Category_3']}.\")\n",
    "value_3 = j['Average_Product_Category_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to Replaces the missing_values with the Averages of their respective column\n",
    "train_data = train_data.fillna({\"Product_ID\":0,\"Product_Category_2\": value_2, \"Product_Category_3\" : value_3})\n",
    "train_data = train_data.withColumn(\"Product_Category_2\", F.col(\"Product_Category_2\").cast(tp.IntegerType()))\n",
    "train_data = train_data.withColumn(\"Product_Category_3\", F.col(\"Product_Category_3\").cast(tp.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID 0\n",
      "Product_ID 0\n",
      "Gender 0\n",
      "Age 0\n",
      "Occupation 0\n",
      "City_Category 0\n",
      "Stay_In_Current_City_Years 0\n",
      "Marital_Status 0\n",
      "Product_Category_1 0\n",
      "Product_Category_2 0\n",
      "Product_Category_3 0\n",
      "Purchase 0\n"
     ]
    }
   ],
   "source": [
    "# null values in each column\n",
    "for c in train_data.columns:\n",
    "    missing_values = F.isnull(c)\n",
    "    missing_values = train_data.filter(missing_values).count()\n",
    "    print(c, missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How many distinct values per column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 145:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|   5891|      3631|     2|  7|        21|            3|                         5|             2|                20|                17|                15|   18105|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# distinct values in each column\n",
    "train_data.agg(*(F.countDistinct(F.col(c)).alias(c) for c in train_data.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Count category values within each of the following column:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Gender| count|\n",
      "+------+------+\n",
      "|     F|135809|\n",
      "|     M|414259|\n",
      "+------+------+\n",
      "\n",
      "+-----+------+\n",
      "|  Age| count|\n",
      "+-----+------+\n",
      "|18-25| 99660|\n",
      "|26-35|219587|\n",
      "| 0-17| 15102|\n",
      "|46-50| 45701|\n",
      "|51-55| 38501|\n",
      "|36-45|110013|\n",
      "|  55+| 21504|\n",
      "+-----+------+\n",
      "\n",
      "+-------------+------+\n",
      "|City_Category| count|\n",
      "+-------------+------+\n",
      "|            B|231173|\n",
      "|            C|171175|\n",
      "|            A|147720|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Gender', 'Age','City_Category']:\n",
    "    variable = train_data.groupBy(c).agg(F.count(c).alias(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------+\n",
      "|Stay_In_Current_City_Years| count|\n",
      "+--------------------------+------+\n",
      "|                         3| 95285|\n",
      "|                         0| 74398|\n",
      "|                        4+| 84726|\n",
      "|                         1|193821|\n",
      "|                         2|101838|\n",
      "+--------------------------+------+\n",
      "\n",
      "+--------------+------+\n",
      "|Marital_Status| count|\n",
      "+--------------+------+\n",
      "|             1|225337|\n",
      "|             0|324731|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Stay_In_Current_City_Years','Marital_Status']:\n",
    "    variable = train_data.groupBy(c).agg(F.count(c).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate average Purchase for each of the following columns:\n",
    "     - Gender\n",
    "     - Age\n",
    "     - City_Category\n",
    "     - Stay_In_Current_City_Years\n",
    "     - Marital_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|Gender|Average_Purchase|\n",
      "+------+----------------+\n",
      "|     F|       8734.5658|\n",
      "|     M|        9437.526|\n",
      "+------+----------------+\n",
      "\n",
      "+-----+----------------+\n",
      "|  Age|Average_Purchase|\n",
      "+-----+----------------+\n",
      "|18-25|       9169.6636|\n",
      "|26-35|       9252.6906|\n",
      "| 0-17|       8933.4646|\n",
      "|46-50|       9208.6257|\n",
      "|51-55|        9534.808|\n",
      "|36-45|       9331.3507|\n",
      "|  55+|       9336.2805|\n",
      "+-----+----------------+\n",
      "\n",
      "+-------------+----------------+\n",
      "|City_Category|Average_Purchase|\n",
      "+-------------+----------------+\n",
      "|            B|       9151.3006|\n",
      "|            C|        9719.921|\n",
      "|            A|       8911.9392|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Gender', 'Age','City_Category']:\n",
    "    (train_data.groupBy(c)).agg(F.round(F.avg('Purchase'),4).alias(\"Average_Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------+\n",
      "|Stay_In_Current_City_Years|Average_Purchase|\n",
      "+--------------------------+----------------+\n",
      "|                         3|       9286.9041|\n",
      "|                         0|       9180.0751|\n",
      "|                        4+|       9275.5989|\n",
      "|                         1|       9250.1459|\n",
      "|                         2|       9320.4298|\n",
      "+--------------------------+----------------+\n",
      "\n",
      "+--------------+----------------+\n",
      "|Marital_Status|Average_Purchase|\n",
      "+--------------+----------------+\n",
      "|             1|       9261.1746|\n",
      "|             0|       9265.9076|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['Stay_In_Current_City_Years','Marital_Status']:\n",
    "    (train_data.groupBy(c)).agg(F.round(F.avg('Purchase'),4).alias(\"Average_Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Label encode the following columns:\n",
    "     - Age\n",
    "     - Gender\n",
    "     - Stay_In_Current_City_Years\n",
    "     - City_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode \n",
    "SI_AGE = StringIndexer(inputCol= \"Age\", outputCol= \"Age_le\" , handleInvalid=\"skip\")\n",
    "SI_Gender = StringIndexer(inputCol= \"Gender\", outputCol= \"Gender_le\", handleInvalid= \"skip\")\n",
    "SI_Stay_In_Current_City_Years  = StringIndexer(inputCol= \"Stay_In_Current_City_Years\", outputCol= \"Stay_In_Current_City_Years_le\", handleInvalid= \"skip\")\n",
    "SI_City_Category = StringIndexer(inputCol= \"City_Category\", outputCol= \"City_Category_le\", handleInvalid= \"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the objects\n",
    "# label encode objects\n",
    "SI_AGE_Obj = SI_AGE.fit(train_data)\n",
    "SI_Gender_Obj = SI_Gender.fit(train_data)\n",
    "SI_Stay_In_Current_City_Years_Obj = SI_Stay_In_Current_City_Years.fit(train_data)\n",
    "SI_City_Category_Obj = SI_City_Category.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform on training data\n",
    "train_data_encoded = SI_AGE_Obj.transform(train_data)\n",
    "train_data_encoded = SI_Gender_Obj.transform(train_data_encoded)\n",
    "train_data_encoded = SI_Stay_In_Current_City_Years_Obj.transform(train_data_encoded)\n",
    "train_data_encoded = SI_City_Category_Obj.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform on test data\n",
    "test_data_encoded = SI_AGE_Obj.transform(test_data)\n",
    "test_data_encoded = SI_Gender_Obj.transform(test_data_encoded)\n",
    "test_data_encoded = SI_Stay_In_Current_City_Years_Obj.transform(test_data_encoded)\n",
    "test_data_encoded = SI_City_Category_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3',\n",
       " 'Purchase',\n",
       " 'Age_le',\n",
       " 'Gender_le',\n",
       " 'Stay_In_Current_City_Years_le',\n",
       " 'City_Category_le',\n",
       " 'Occupation_le']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_encoded.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. One-Hot encode following columns:\n",
    "     - Gender\n",
    "     - City_Category\n",
    "     - Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Label encoding Occupation\n",
    "SI_Occupation = StringIndexer(inputCol= \"Occupation\", outputCol= \"Occupation_le\", handleInvalid= \"skip\")\n",
    "SI_Occupation_Obj = SI_Occupation.fit(train_data)\n",
    "train_data_encoded = SI_Occupation_Obj.transform(train_data_encoded)\n",
    "test_data_encoded = SI_Occupation_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "OHE_train = OneHotEncoder(inputCols=['Gender_le','City_Category_le','Occupation_le'],\n",
    "                                  outputCols=['Gender_le_ohe','City_Category_le_ohe','Occupation_le_ohe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE object\n",
    "OHE_Obj = OHE_train.fit(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform train data\n",
    "train_data_encoded = OHE_Obj.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-------------+--------------------+-----------------+\n",
      "|Marital_Status|Age_le|Gender_le_ohe|City_Category_le_ohe|Occupation_le_ohe|\n",
      "+--------------+------+-------------+--------------------+-----------------+\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   6.0|    (1,[],[])|           (2,[],[])|  (20,[12],[1.0])|\n",
      "|             0|   5.0|(1,[0],[1.0])|       (2,[1],[1.0])|   (20,[9],[1.0])|\n",
      "|             0|   0.0|(1,[0],[1.0])|           (2,[],[])|  (20,[14],[1.0])|\n",
      "|             1|   3.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[2],[1.0])|\n",
      "|             1|   3.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[2],[1.0])|\n",
      "|             1|   3.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[2],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|           (2,[],[])|   (20,[5],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             0|   4.0|    (1,[],[])|           (2,[],[])|  (20,[19],[1.0])|\n",
      "|             1|   1.0|(1,[0],[1.0])|       (2,[0],[1.0])|   (20,[3],[1.0])|\n",
      "|             1|   0.0|(1,[0],[1.0])|       (2,[1],[1.0])|   (20,[6],[1.0])|\n",
      "+--------------+------+-------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the one hot encoded data\n",
    "train_data_encoded.select('Marital_Status','Age_le','Gender_le_ohe','City_Category_le_ohe','Occupation_le_ohe').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = OHE_Obj.transform(test_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Build a baseline model using any of the ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3',\n",
       " 'Purchase',\n",
       " 'Age_le',\n",
       " 'Gender_le',\n",
       " 'Stay_In_Current_City_Years_le',\n",
       " 'City_Category_le',\n",
       " 'Occupation_le',\n",
       " 'Gender_le_ohe',\n",
       " 'City_Category_le_ohe',\n",
       " 'Occupation_le_ohe']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## columns in the dataset\n",
    "train_data_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create feature vector\n",
    "feature_vector = VectorAssembler(inputCols= ['User_ID',\n",
    " 'Product_ID',\n",
    " 'Marital_Status',\n",
    " 'Product_Category_1',\n",
    " 'Product_Category_2',\n",
    " 'Product_Category_3',\n",
    " 'Age_le',\n",
    " 'Stay_In_Current_City_Years_le',\n",
    " 'Gender_le_ohe',\n",
    " 'City_Category_le_ohe',\n",
    " 'Occupation_le_ohe'], outputCol= 'feature_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column Product_ID is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/PySpark/6 - Spark ML/Spark_ML_Assignment/Spark_ML_Assignment/Ml_Assignment.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bneerajsirvisetti-pyspark-q6jxqq46f4qwp/workspaces/PySpark/6%20-%20Spark%20ML/Spark_ML_Assignment/Spark_ML_Assignment/Ml_Assignment.ipynb#ch0000041vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# transform the feature vector\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bneerajsirvisetti-pyspark-q6jxqq46f4qwp/workspaces/PySpark/6%20-%20Spark%20ML/Spark_ML_Assignment/Spark_ML_Assignment/Ml_Assignment.ipynb#ch0000041vscode-remote?line=1'>2</a>\u001b[0m train_data_encoded \u001b[39m=\u001b[39m feature_vector\u001b[39m.\u001b[39;49mtransform(train_data_encoded)\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(dataset)\n\u001b[1;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/pyspark/ml/wrapper.py:396\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 396\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mtransform(dataset\u001b[39m.\u001b[39;49m_jdf), dataset\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type string of column Product_ID is not supported."
     ]
    }
   ],
   "source": [
    "# transform the feature vector\n",
    "train_data_encoded = feature_vector.transform(train_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
