{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Questions\n",
    "1. What are the distinct number of meal categories andcuisines?\n",
    "2. Which center_id has the highest num_orders?\n",
    "3. What is the top selling cuisine at the center_id thathad the highest num_orders?\n",
    "4. What is the average op_area per center_type?\n",
    "5. Which center_type had the highest revenue? (Revenueis total sum of\n",
    "checkout_price*num_orders)\n",
    "6. Which is the top ordered cuisine in terms of num_orders?\n",
    "7. What are the num_orders per cuisine per week?\n",
    "8. Which center_id gave the highest number of discounts?(Discount is considered\n",
    "when checkout_price is less than base_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/03 17:47:09 WARN Utils: Your hostname, codespaces-ae9593 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "22/07/03 17:47:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/03 17:47:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/03 17:47:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://73cdfaef-3caa-4c7b-ab2b-de19aa9c1321.internal.cloudapp.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4e55352080>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "demand_data = spark.read.csv('data/train.csv',inferSchema=True,header=True) # Creating dataframe from the data\n",
    "\n",
    "demand_data.createOrReplaceTempView(\"demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- center_id: integer (nullable = true)\n",
      " |-- meal_id: integer (nullable = true)\n",
      " |-- checkout_price: double (nullable = true)\n",
      " |-- base_price: double (nullable = true)\n",
      " |-- emailer_for_promotion: integer (nullable = true)\n",
      " |-- homepage_featured: integer (nullable = true)\n",
      " |-- num_orders: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demand_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+-------+--------------+----------+---------------------+-----------------+----------+\n",
      "|     id|week|center_id|meal_id|checkout_price|base_price|emailer_for_promotion|homepage_featured|num_orders|\n",
      "+-------+----+---------+-------+--------------+----------+---------------------+-----------------+----------+\n",
      "|1379560|   1|       55|   1885|        136.83|    152.29|                    0|                0|       177|\n",
      "|1466964|   1|       55|   1993|        136.83|    135.83|                    0|                0|       270|\n",
      "|1346989|   1|       55|   2539|        134.86|    135.86|                    0|                0|       189|\n",
      "|1338232|   1|       55|   2139|         339.5|    437.53|                    0|                0|        54|\n",
      "|1448490|   1|       55|   2631|         243.5|     242.5|                    0|                0|        40|\n",
      "|1270037|   1|       55|   1248|        251.23|    252.23|                    0|                0|        28|\n",
      "|1191377|   1|       55|   1778|        183.36|    184.36|                    0|                0|       190|\n",
      "|1499955|   1|       55|   1062|        182.36|    183.36|                    0|                0|       391|\n",
      "|1025244|   1|       55|   2707|        193.06|    192.06|                    0|                0|       472|\n",
      "|1054194|   1|       55|   1207|        325.92|    384.18|                    0|                1|       676|\n",
      "|1469367|   1|       55|   1230|        323.01|     390.0|                    0|                1|       823|\n",
      "|1029333|   1|       55|   2322|        322.07|     388.0|                    0|                1|       972|\n",
      "|1446016|   1|       55|   2290|        311.43|    310.43|                    0|                0|       162|\n",
      "|1244647|   1|       55|   1727|        445.23|    446.23|                    0|                0|       420|\n",
      "|1378227|   1|       55|   1109|        264.84|    297.79|                    1|                0|       756|\n",
      "|1181556|   1|       55|   2640|        282.33|    281.33|                    0|                0|       108|\n",
      "|1313873|   1|       55|   2306|         243.5|    340.53|                    0|                0|        28|\n",
      "|1067069|   1|       55|   2126|         486.0|     485.0|                    0|                0|        28|\n",
      "|1058482|   1|       55|   2826|        306.58|    305.58|                    0|                0|       188|\n",
      "|1240935|   1|       55|   1754|        289.12|    289.12|                    0|                0|       485|\n",
      "+-------+----+---------+-------+--------------+----------+---------------------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from demand\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullfillment_centre = spark.read.csv('data/fulfilment_center_info.csv',inferSchema=True,header=True) # Creating dataframe from the data\n",
    "\n",
    "fullfillment_centre.createOrReplaceTempView(\"centre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- center_id: integer (nullable = true)\n",
      " |-- city_code: integer (nullable = true)\n",
      " |-- region_code: integer (nullable = true)\n",
      " |-- center_type: string (nullable = true)\n",
      " |-- op_area: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullfillment_centre.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------+-------+\n",
      "|center_id|city_code|region_code|center_type|op_area|\n",
      "+---------+---------+-----------+-----------+-------+\n",
      "|       11|      679|         56|     TYPE_A|    3.7|\n",
      "|       13|      590|         56|     TYPE_B|    6.7|\n",
      "|      124|      590|         56|     TYPE_C|    4.0|\n",
      "|       66|      648|         34|     TYPE_A|    4.1|\n",
      "|       94|      632|         34|     TYPE_C|    3.6|\n",
      "|       64|      553|         77|     TYPE_A|    4.4|\n",
      "|      129|      593|         77|     TYPE_A|    3.9|\n",
      "|      139|      693|         34|     TYPE_C|    2.8|\n",
      "|       88|      526|         34|     TYPE_A|    4.1|\n",
      "|      143|      562|         77|     TYPE_B|    3.8|\n",
      "|      101|      699|         85|     TYPE_C|    2.8|\n",
      "|       86|      699|         85|     TYPE_C|    4.0|\n",
      "|       32|      526|         34|     TYPE_A|    3.8|\n",
      "|      149|      478|         77|     TYPE_A|    2.4|\n",
      "|      152|      576|         34|     TYPE_B|    4.0|\n",
      "|       92|      526|         34|     TYPE_C|    2.9|\n",
      "|       27|      713|         85|     TYPE_A|    4.5|\n",
      "|       14|      654|         56|     TYPE_C|    2.7|\n",
      "|       26|      515|         77|     TYPE_C|    3.0|\n",
      "|      104|      647|         56|     TYPE_A|    4.5|\n",
      "+---------+---------+-----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from centre\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_info = spark.read.csv('data/meal_info.csv',inferSchema=True,header=True) # Creating dataframe from the data\n",
    "\n",
    "meal_info.createOrReplaceTempView(\"meal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- meal_id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meal_info.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------+\n",
      "|meal_id|    category|cuisine|\n",
      "+-------+------------+-------+\n",
      "|   1885|   Beverages|   Thai|\n",
      "|   1993|   Beverages|   Thai|\n",
      "|   2539|   Beverages|   Thai|\n",
      "|   1248|   Beverages| Indian|\n",
      "|   2631|   Beverages| Indian|\n",
      "|   1311|      Extras|   Thai|\n",
      "|   1062|   Beverages|Italian|\n",
      "|   1778|   Beverages|Italian|\n",
      "|   1803|      Extras|   Thai|\n",
      "|   1198|      Extras|   Thai|\n",
      "|   2707|   Beverages|Italian|\n",
      "|   1847|        Soup|   Thai|\n",
      "|   1438|        Soup|   Thai|\n",
      "|   2494|        Soup|   Thai|\n",
      "|   2760|Other Snacks|   Thai|\n",
      "|   2490|       Salad|Italian|\n",
      "|   1109|   Rice Bowl| Indian|\n",
      "|   2290|   Rice Bowl| Indian|\n",
      "|   1525|Other Snacks|   Thai|\n",
      "|   2704|Other Snacks|   Thai|\n",
      "+-------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from meal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the distinct number of meal categories and cuisines?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distinct no of meal categories are:  14\n",
      "+------------+\n",
      "|    category|\n",
      "+------------+\n",
      "|       Salad|\n",
      "|      Desert|\n",
      "|     Biryani|\n",
      "|   Rice Bowl|\n",
      "|    Sandwich|\n",
      "|       Pizza|\n",
      "|   Beverages|\n",
      "|Other Snacks|\n",
      "|        Soup|\n",
      "|    Starters|\n",
      "|      Extras|\n",
      "|     Seafood|\n",
      "|       Pasta|\n",
      "|        Fish|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1. What are the distinct number of meal categories\n",
    "print(\"The distinct no of meal categories are: \",spark.sql(\"select distinct(category) from meal\").count())\n",
    "spark.sql(\"select distinct(category) from meal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distinct no of meal cusines are:  4\n",
      "+-----------+\n",
      "|    cuisine|\n",
      "+-----------+\n",
      "|       Thai|\n",
      "|     Indian|\n",
      "|Continental|\n",
      "|    Italian|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1. What are the distinct number of cuisines \n",
    "print(\"The distinct no of meal cusines are: \", spark.sql(\"select distinct(cuisine) from meal\").count())\n",
    "spark.sql(\"select distinct(cuisine) from meal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Which center_id has the highest num_orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Centre with the highest orders is 13, with total orders = 1742220\n",
      "+---------+------------+\n",
      "|center_id|Total_Orders|\n",
      "+---------+------------+\n",
      "|       13|     1742220|\n",
      "+---------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k=(demand_data.groupBy(\"center_id\").agg(F.sum(\"num_orders\").alias(\"Total_Orders\"))).orderBy(\"Total_Orders\", ascending=False)\n",
    "j = ((k.collect())[0]).asDict()\n",
    "print(f\"The Centre with the highest orders is {j['center_id']}, with total orders = {j['Total_Orders']}\")\n",
    "k.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|center_id|Total_Orders|\n",
      "+---------+------------+\n",
      "|       13|     1742220|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative way\n",
    "statmt = '''\n",
    "SELECT center_id,SUM(num_orders) AS Total_Orders\n",
    "FROM demand\n",
    "GROUP BY center_id\n",
    "ORDER BY SUM(num_orders) DESC\n",
    "LIMIT 1\n",
    "'''\n",
    "k = spark.sql(statmt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the top selling cuisine at the center_id that had the highest num_orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top selling Cuisine being Indian , the Centre with the highest orders is 43, with total orders = 24299\n",
      "+---------+----------+-------+\n",
      "|center_id|num_orders|cuisine|\n",
      "+---------+----------+-------+\n",
      "|       43|     24299| Indian|\n",
      "|       43|     15336| Indian|\n",
      "|       43|     14229| Indian|\n",
      "|       10|     13580| Indian|\n",
      "|       89|     12489| Indian|\n",
      "|      146|     12327|Italian|\n",
      "|      137|     12177| Indian|\n",
      "|       99|     12137| Indian|\n",
      "|      126|     11380|Italian|\n",
      "|       11|     11260| Indian|\n",
      "|      104|     11246| Indian|\n",
      "|       13|     10745| Indian|\n",
      "|       51|     10449| Indian|\n",
      "|       52|     10435| Indian|\n",
      "|      108|     10259| Indian|\n",
      "|       99|     10193| Indian|\n",
      "|      113|     10124| Indian|\n",
      "|       59|      9828| Indian|\n",
      "|       99|      9815| Indian|\n",
      "|      113|      9532|Italian|\n",
      "+---------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statmt = '''\n",
    "SELECT center_id,num_orders,cuisine \n",
    "FROM demand,meal \n",
    "WHERE demand.meal_id = meal.meal_id \n",
    "ORDER BY num_orders DESC\n",
    "'''\n",
    "k = spark.sql(statmt)\n",
    "j = ((k.collect())[0]).asDict()\n",
    "print(f\"The Top selling Cuisine being {j['cuisine']} , the Centre with the highest orders is {j['center_id']}, with total orders = {j['num_orders']}\")\n",
    "k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the average op_area per center_type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|center_type|       AVG_OP_AREA|\n",
      "+-----------+------------------+\n",
      "|     TYPE_C|3.1578947368421044|\n",
      "|     TYPE_B|4.7733333333333325|\n",
      "|     TYPE_A| 4.076744186046512|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statmt = '''\n",
    "SELECT center_type, AVG(op_area) AS AVG_OP_AREA\n",
    "FROM centre\n",
    "GROUP BY center_type\n",
    "'''\n",
    "k = spark.sql(statmt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Which center_type had the highest revenue? (Revenue is total sum of checkout_price*num_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Center Type which has the highest revenue is TYPE_A, the revenue being 7276203201.870064\n",
      "+-----------+-------------------+\n",
      "|center_type|            Revenue|\n",
      "+-----------+-------------------+\n",
      "|     TYPE_A|7.276203201870064E9|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statmt = '''\n",
    "SELECT c.center_type,SUM(d.checkout_price * d.num_orders) AS Revenue\n",
    "FROM centre c\n",
    "JOIN demand d ON c.center_id = d.center_id\n",
    "GROUP BY c.center_type\n",
    "ORDER BY Revenue DESC\n",
    "LIMIT 1\n",
    "'''\n",
    "k = spark.sql(statmt)\n",
    "j = ((k.collect())[0]).asDict()\n",
    "print(f\"The Center Type which has the highest revenue is {j['center_type']}, the revenue being {j['Revenue']}\")\n",
    "k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|center_type|            Revenue|\n",
      "+-----------+-------------------+\n",
      "|     TYPE_A|7.276203201870064E9|\n",
      "|     TYPE_B|3.172968529400033E9|\n",
      "|     TYPE_C|2.251833991370006E9|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate way\n",
    "statmt = '''\n",
    "SELECT center_type, SUM(checkout_price*num_orders) AS Revenue\n",
    "FROM centre,demand\n",
    "WHERE centre.center_id = demand.center_id\n",
    "GROUP BY center_type\n",
    "ORDER BY Revenue DESC\n",
    "'''\n",
    "k = spark.sql(statmt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Which is the top ordered cuisine in terms of num_orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tope Ordered Cuisine is Italian, the total No of Orders being 17166334\n",
      "+-----------+------------+\n",
      "|    cuisine|Total_Orders|\n",
      "+-----------+------------+\n",
      "|    Italian|    17166334|\n",
      "|       Thai|    14058488|\n",
      "|     Indian|    10979934|\n",
      "|Continental|     6766188|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statmt = '''\n",
    "SELECT m.cuisine, SUM(d.num_orders) AS Total_Orders\n",
    "FROM meal m\n",
    "JOIN demand d ON m.meal_id = d.meal_id\n",
    "GROUP BY m.cuisine\n",
    "ORDER BY SUM(d.num_orders) DESC\n",
    "'''\n",
    "k = spark.sql(statmt)\n",
    "j = ((k.collect())[0]).asDict()\n",
    "print(f\"The Tope Ordered Cuisine is {j['cuisine']}, the total No of Orders being {j['Total_Orders']}\")\n",
    "k.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What are the num_orders per cuisine per week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+\n",
      "|week|    cuisine|Total_Orders|\n",
      "+----+-----------+------------+\n",
      "|   1|Continental|      146020|\n",
      "|   1|     Indian|      175317|\n",
      "|   1|    Italian|      228836|\n",
      "|   1|       Thai|      242088|\n",
      "|   2|     Indian|      177109|\n",
      "|   2|       Thai|      273778|\n",
      "|   2|Continental|      133570|\n",
      "|   2|    Italian|      202627|\n",
      "|   3|    Italian|      197299|\n",
      "|   3|     Indian|      150148|\n",
      "|   3|       Thai|      249838|\n",
      "|   3|Continental|       97977|\n",
      "|   4|    Italian|      192265|\n",
      "|   4|       Thai|      277206|\n",
      "|   4|     Indian|      155239|\n",
      "|   4|Continental|      118819|\n",
      "|   5|Continental|      116077|\n",
      "|   5|    Italian|      169161|\n",
      "|   5|     Indian|      683532|\n",
      "|   5|       Thai|      229905|\n",
      "+----+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statmt = '''\n",
    "SELECT d.week, m.cuisine, SUM(d.num_orders) AS Total_Orders\n",
    "FROM meal m\n",
    "JOIN demand d ON m.meal_id = d.meal_id\n",
    "GROUP BY d.week,m.cuisine\n",
    "ORDER BY d.week\n",
    "'''\n",
    "k = spark.sql(statmt).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Which center_id gave the highest number of discounts?(Discount is considered when checkout_price is less than base_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Center Id for which there was highest no of Discounts is 13, they gave discounts a total of 1509 times, the Total Discount they gave being 61917.64000000004.\n",
      "+---------+------------------+---------------+\n",
      "|center_id|    Total_Discount|No_Of_Discounts|\n",
      "+---------+------------------+---------------+\n",
      "|       13| 61917.64000000004|           1509|\n",
      "|       30| 71262.66000000002|           1495|\n",
      "|      137| 60545.02000000001|           1462|\n",
      "|       27| 63096.98000000007|           1462|\n",
      "|      153| 60795.27000000003|           1455|\n",
      "|       10|58329.800000000025|           1454|\n",
      "|       51|63354.630000000034|           1453|\n",
      "|      174|64248.720000000016|           1447|\n",
      "|       36|58773.280000000006|           1445|\n",
      "|      104|          59457.86|           1443|\n",
      "|       11| 60545.38999999998|           1439|\n",
      "|      132|62439.079999999994|           1435|\n",
      "|       43|60019.820000000014|           1431|\n",
      "|      108| 61210.79000000001|           1429|\n",
      "|       89| 62003.43000000002|           1425|\n",
      "|       20|          59968.68|           1418|\n",
      "|       52| 59970.04999999996|           1413|\n",
      "|       67|59025.260000000024|           1404|\n",
      "|       59|59970.760000000024|           1399|\n",
      "|       23|62491.849999999984|           1393|\n",
      "+---------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statmt = '''\n",
    "SELECT center_id, SUM(base_price - checkout_price) AS Total_Discount, COUNT(*) AS No_Of_Discounts\n",
    "FROM demand\n",
    "WHERE base_price > checkout_price\n",
    "GROUP BY center_id\n",
    "ORDER BY COUNT(*) DESC\n",
    "'''\n",
    "k = spark.sql(statmt)\n",
    "j = ((k.collect())[0]).asDict()\n",
    "print(f\"The Center Id for which there was highest no of Discounts is {j['center_id']}, they gave discounts a total of {j['No_Of_Discounts']} times, the Total Discount they gave being {j['Total_Discount']}.\")\n",
    "k.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
